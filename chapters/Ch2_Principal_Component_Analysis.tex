\documentclass[../main.tex]{subfiles}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   New Chapter                                       %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Principal Component Analysis}
In the last chapter, we discussed linear dimension reduction via linear autoencoder. Now we try to understand linear dimension reduction in a principled manner. We start with the 1-dimension case, i.e. we want to approximate a group of points via points on a line. Then we generalize it to multiple dimensions, which yields exactly the idea of PCA. Finally we introduce a simple algorithm to find the dominant eigenvector of a matrix, followed by some examples and comparison with Linear Autoencoder which we discussed in the last chapter.
\section{1D Linear Case}
\subsection{Line in $\mathbb{R}^m$}
Mathematically, we can represent a line in $\mathbb{R}^m$ in the following parametric form:
\begin{equation*}
\bm{\mu} + \mathbb{R}\bm{u} \equiv \{\bm{v}\in \mathbb{R}^m:\exists z\in \mathbb{R}\ s.t.\ \bm{v}=\bm{\mu} + z\bm{u} \}
\end{equation*}
where $\bm{\mu}$ is the offset or shift, $\bm{u}$ is the direction vector such that $\|\bm{u}\|=1$. Note that representation is not unique since a scaling on $\bm{u}$ and a shifting on $\bm{\mu}$ along $\bm{u}$ will not change the line. 
\subsection{Approximation of a single point: orthogonal projection}
Now consider the task of approximating a data point $\bm{x}\in\mathbb{R}^m$ by a point on the line. It is natural to request that our approximation should be close to the original point in Euclidean distance. Formally, we try to solve
\begin{equation}\label{eq_2_1}
\mathop{\arg\min}_{z\in\mathbb{R}}\|\bm{\mu}+z\bm{u}-\bm{x}\|^2
\end{equation}
or equivalently,
\begin{equation*}
\mathop{\arg\min}_{\hat{\bm{x}}\in \bm{\mu} + \mathbb{R}\bm{u}}\|\hat{\bm{x}}-\bm{x}\|^2
\end{equation*}
To solve (\ref{eq_2_1}), we request the first order optimality condition:
\begin{equation*}
\frac{d}{dz}\|\bm{\mu}+z\bm{u}-\bm{x}\|^2 = 2\langle \bm{\mu}+z\bm{u}-\bm{x}, \bm{u}\rangle \overset{!}{=}0
\end{equation*}
this yields solutions
\begin{align*}
z &= \langle \bm{x}-\bm{\mu},\bm{u} \rangle\\
\hat{\bm{x}} &= \bm{\mu} + \langle \bm{x}-\bm{\mu},\bm{u} \rangle \bm{u}
\end{align*}
which is the \emph{orthogonal projection} of $\bm{x}$ to the line. 

\subsection{Optimal Line for multiple points}
Assume we are given data points $\{\bm{x}_1,\dots,\bm{x}_n\}\subset \mathbb{R}^m$. With the orthogonal projection result, the optimal approximation of $\{\bm{x}_i\}$ by a line $(\bm{u},\bm{\mu})$ is given by $\{\bm{\mu} + \langle \bm{x}_i-\bm{\mu},\bm{u} \rangle\}$. Therefore the optimal line can be found by solving
\begin{equation*}
\mathop{\arg\min}_{(\bm{u},\bm{\mu})} \left[ \frac{1}{2n}\sum_{i=1}^n \| \bm{\mu}+ \langle \bm{x}_i-\bm{\mu},\bm{u\rangle\bm{u} - \bm{x}_i} \|^2\right]
\end{equation*}
using that $\langle\bm{v},\bm{u}\rangle\bm{u}=(\bm{u}\bm{u}^T)\bm{v}$, it's equivalent to
\begin{equation}\label{eq_2_obj}
\mathop{\arg\min}_{(\bm{u},\bm{\mu})} \left[ \frac{1}{2n}\sum_{i=1}^n \left\| \left(I-\bm{u}\bm{u}^T \right)(\bm{x}_i-\bm{\mu}) \right\| ^2\right]
\end{equation}
Matrix $\left(I-\bm{u}\bm{u}^T \right)$ acts as a linear map, more specifically, as a projection. Take an argument $\bm{v}$, by associativity, we get
\begin{equation*}
\left(I-\bm{u}\bm{u}^T \right)\bm{v} = \bm{v} - \langle \bm{u}, \bm{v}\rangle\bm{u}
\end{equation*}
The RHS is the vector itself minus the projection to the line $\mathbb{R}\bm{u}$, which is the projection to the orthogonal complement $(\mathbb{R}\bm{u})^{\bot}$. Also, this operation is \emph{idempotent}, i.e. projecting a second time gives the same result, since
\begin{equation*}
\left(I-\bm{u}\bm{u}^T \right)( \bm{v} - \langle \bm{u}, \bm{v}\rangle\bm{u}) =  \bm{v} - \langle \bm{u}, \bm{v}\rangle\bm{u} - ( \langle \bm{u}, \bm{v}\rangle\bm{u} -  \langle \bm{u}, \bm{v}\rangle\bm{u}) =  \bm{v} - \langle \bm{u}, \bm{v}\rangle\bm{u}
\end{equation*}
To solve (\ref{eq_2_obj}), we first request the first order optimality condition for $\bm{\mu}$
\begin{align*}
&\nabla_{\bm{\mu}} \left[ \frac{1}{2n}\sum_{i=1}^n \left\| \left(I-\bm{u}\bm{u}^T \right)(\bm{x}_i-\bm{\mu}) \right\| ^2\right] \overset{!}{=} 0\\
\Longleftrightarrow ~&  \frac{1}{n}\sum_{i=1}^n  \left(I-\bm{u}\bm{u}^T \right)(\bm{x}_i-\bm{\mu}) \overset{!}{=} 0\\
\Longleftrightarrow ~& \left(I-\bm{u}\bm{u}^T \right)(\frac{1}{n}\sum_{i=1}^n\bm{x}_i-\bm{\mu})\overset{!}{=} 0
\end{align*}
Note that $\|\bm{u}\|=1$, then $\bm{u}$ is an eigenvector of $I-\bm{u}\bm{u}^T $ with corresponding eigenvalue $0$. From this we see that the optimal solution for $\bm{\mu}$ is not unique. Any $\bm{\mu}$ satisfying 
\begin{equation*}
\frac{1}{n}\sum_{i=1}^n\bm{x}_i-\bm{\mu} = k\bm{u},\ k\in \mathbb{R}
\end{equation*}
will be an optimal solution. We can also see this from another intuitive angle: any $\bm{\mu}$ that lies on the optimal line with the same direction determines the same line.
\par Although the first order optimality condition cannot determine $\bm{\mu}$ uniquely, there is a unique solution for all $\bm{u}$:
\begin{equation*}
\bm{\mu} = \frac{1}{n}\sum_{i=1}^n\bm{x}_i ~\equiv~ sample\ mean
\end{equation*}
which means that all the lines that are optimal in this 1-D setting have to pass through $ \bm{\mu} = \frac{1}{n}\sum_{i=1}^n\bm{x}_i$. Since it will be good to have a "correct" $\bm{\mu}$ that can guarantee optimality without the dependency on $\bm{u}$, we simply choose $\bm{\mu}$ to be the sample mean.
\par Choosing $\bm{\mu} = \frac{1}{n}\sum_{i=1}^n\bm{x}_i$ is often referred to as \emph{centering} the data in the ML literature. By centering the data, i.e.
\begin{equation*}
\bm{x}_i\leftarrow \bm{x}_i-\frac{1}{n}\sum_{i=1}^n\bm{x}_i
\end{equation*}
we identify the center of mass with the origin and restrict the problem to linear (instead of affine) subspaces. This will simplify the derivations and analyses without loss in modeling power. Therefore from now on, w.l.o.g. we assume that the data points are centered.
\par By centering the data, the optimization problem is left with
\begin{align}
&\mathop{\arg\min}_{\|\bm{u}\|=1} \left[ \frac{1}{n}\sum_{i=1}^n \|\langle \bm{x}_i,\bm{u}\rangle \bm{u} - \bm{x}_i\|^2 \right]\notag\\
\Longleftrightarrow &\mathop{\arg\min}_{\|\bm{u}\|=1} \left[ \frac{1}{n}\sum_{i=1}^n \|\langle \bm{x}_i,\bm{u}\rangle \bm{u}\|^2 + \|\bm{x}_i\|^2 - 2\langle \bm{x}_i,\bm{u}\rangle^2 \right]\notag\\
\Longleftrightarrow &\mathop{\arg\min}_{\|\bm{u}\|=1} \left[- \frac{1}{n}\sum_{i=1}^n\langle \bm{x}_i,\bm{u}\rangle^2 \right]\ (\bm{x}_i\text{ is a const})\notag\\
\Longleftrightarrow &\mathop{\arg\min}_{\|\bm{u}\|=1} \left[- \frac{1}{n}\sum_{i=1}^n\ \bm{u}^T\bm{x}_i\bm{x}_i^T\bm{u} \right]\notag\\
\label{eq_2_obj_u}\Longleftrightarrow &\mathop{\arg\min}_{\|\bm{u}\|=1} \left[- \bm{u}^T\left(\frac{1}{n}\sum_{i=1}^n\ \bm{x}_i\bm{x}_i^T\right)\bm{u} \right]\ (\text{Associativity})
\end{align}
where the matrix between $\bm{u}^T$ and $\bm{u}$ is call \emph{variance-covariance matrix} of the (centered) data samples
\begin{equation*}
\bm{\Sigma} \equiv \frac{1}{n}\sum_{i=1}^n\ \bm{x}_i\bm{x}_i^T = \frac{1}{n}{\bf XX}^T \in \mathbb{R}^{m\times m},\ {\bf X}\equiv[\bm{x}_1,\dots,\bm{x}_n]
\end{equation*}
This gives us a very important insight: in the 1-D setting, $\bm{u}$ depends on the data only through ${\bf \Sigma}$. This is remarkable, since the dependency between $\bm{u}$ and all the data points ${\bf X}$ can be very complicated and there are a lot of information in ${\bf X}$. But (\ref{eq_2_obj_u}) tells us the only thing matters is ${\bf \Sigma}$. Suppose we want to fit the data in a streaming manner, the above analysis shows that we can update $\bm{u}$ very efficiently by recomputing ${\bf \Sigma}$.
\par Problem (\ref{eq_2_obj_u}) is a constrained optimization problem. To solve it we introduce the Lagrange multiplier $\lambda$ and define the following Lagrangian
\begin{equation}\label{eq_1_lag}
L(\bm{u},\lambda) = -\bm{u}^T\bm{\Sigma u} + \lambda (\bm{u}^T\bm{u}-1)
\end{equation}
Before trying to solve it, we first take a duality view and gain some insight into the Lagrange method. Assume we have a primal player who wants to minimize the objective in (\ref{eq_2_obj_u}). To remove the constraint on $\bm{u}$, we introduce a dual player who wants to maximize the objective and would exploit a deviation from the original constraint on $\bm{u}$. Formally, we consider
\begin{equation}\label{eq_2_dual}
\min_{\bm{u}}\max_{\lambda} -\bm{u}^T\bm{\Sigma u} + \lambda (\bm{u}^T\bm{u}-1)
\end{equation}
If the primal player (picking $\bm{u}$ and attempting to minimize the objective) picks $\bm{u}$ that does not satisfy the original constraint, the dual player (picking $\lambda$ and trying to maximizing the objective) will be able to drive the objective value to $+\infty$ since there's no constraint on $\lambda$. So it is clear that (\ref{eq_2_dual}) is equivalent to (\ref{eq_2_obj_u}).
\par With this intuition, we now try to solve (\ref{eq_1_lag}) by again applying the first order optimality condition:
\begin{equation*}
\nabla_{\bm{u}} L(\bm{u},\lambda) = -2(\bm{\Sigma u}-\lambda \bm{u})\overset{!}{=}0 \Longleftrightarrow \bm{\Sigma u}=\lambda \bm{u}
\end{equation*}
This means that minimizing over $\bm{u}$ requires $\bm{u}$ to be an eigenvector of ${\bf \Sigma}$ with the corresponding eigenvalue $\lambda$. Since we want to maximize over $\lambda$, $\bm{u}$ should be a \textbf{principal} eigenvector of $\bm{\Sigma}$. So the optimal direction of the line is the principal eigenvector of the sample variance-covariance matrix. Also we have the following extremal characterization
\begin{equation*}
\bm{u} = \mathbb{\arg\max}_{\bm{v}:\|\bm{v}\|=1}\left[\bm{v}^T\bm{\Sigma v} \right]
\end{equation*}
\begin{remark}\label{rmk_2_1}
	Usually, even if we can mathematically define and find the eigenvectors and eigenvalues of a matrix, it is still hard to interpret what these vectors and values are. The discussion above give us some insight on the principal eigenvector (of the variance-covariance matrix of centered data) in terms of 1-D PCA. It is the direction along which the projected data have the smallest reconstruction error, and also the direction of largest data variance (we will see this soon).
\end{remark}
\par Before we finish this section, we re-interpret the 1-D PCA problem, as suggested in the end of Remark (\ref{rmk_2_1}), in terms of variance maximization in 1-D representation. 
\par Consider the variance of the projected data (remember that we assume centered data, which means the projections (a linear transformation) will also be centered)
\begin{equation*}
\text{Var}[z]=\frac{1}{n}\sum_{i=1}^{n}z_i^2=\frac{1}{n}\sum_{i=1}^{n}\langle \bm{x}_i,\bm{u}\rangle^2 = \bm{u}^T\bm{\Sigma u}
\end{equation*}
This shows the equivalence of
\begin{itemize}
	\item Direction of the smallest reconstruction error.
	\item Direction of the largest data variance.
\end{itemize}
\section{Principal Component Analysis}
Define the residual as the projection of the data to $(\mathbb{R}\bm{u})^{\bot}$:
\begin{equation*}
\bm{r}_i:=\bm{x}_i - \langle\bm{x}_i,\bm{u}  \rangle\bm{u} = \left({\bf I} - \bm{uu}^T \right)\bm{x}_i
\end{equation*}
The variance-covariance matrix of the residual vectors (also centered) is
\begin{align*}
\frac{1}{n}\sum_{i=1}^n \bm{r}_i\bm{r}_i^T &=  \frac{1}{n}\sum_{i=1}^n \left({\bf I} - \bm{uu}^T \right)\bm{x}_i\bm{x}_i^T\left({\bf I} - \bm{uu}^T \right)\\
&= \left({\bf I} - \bm{uu}^T \right)\bm{\Sigma}\left({\bf I} - \bm{uu}^T \right)\\
&=\bm{\Sigma} - \lambda\bm{uu}^T\ (\bm{\Sigma u}=\lambda \bm{u},\ \bm{u}^T\bm{u}=1)
\end{align*}
This matrix has some interesting properties. First, $\bm{u}$ is its eigenvector with eigenvalue $0$:
\begin{equation*}
\left(\bm{\Sigma} - \lambda\bm{uu}^T\right)\bm{u} = \bm{\Sigma u}-\lambda \bm{u}=0
\end{equation*}
Recall the eigen-decomposition of $\bm{\Sigma}$
\begin{equation}\label{eq_2_edom}
\bm{\Sigma} = \sum_{i=1}^m \lambda_i\bm{u}_i\bm{u}_i^T
\end{equation}
where $\lambda_1\geq\dots\geq\lambda_m\geq 0$ are the eigenvalues of $\Sigma$ (the non-negativity is from that $\Sigma$ is positive semi-definite (p.s.d.)) and the $\bm{u}_i$'s are corresponding eigenvectors. Also we have $\lambda = \lambda_1$ and $\bm{u}=\bm{u}_1$ by the discussion so far. Therefore, we obtain
\begin{equation*}
\bm{\Sigma} - \lambda\bm{uu}^T = \sum_{i=1}^m \lambda_i\bm{u}_i\bm{u}_i^T - \lambda_1\bm{u}_1\bm{u}_1^T = \sum_{i=2}^m \lambda_i\bm{u}_i\bm{u}_i^T
\end{equation*}
which suggests that the eigenvectors of $\bm{\Sigma} - \lambda\bm{uu}^T$ are still the eigenvectors of $\bm{\Sigma}$, so are the eigenvalues expect that the eigenvalue of $\bm{u}_1$ changes to $0$. This means that the principal eigenvector of $\bm{\Sigma} - \lambda\bm{uu}^T$ is the second principal eigenvector of $\bm{\Sigma}$. Moreover, the second principal eigenvector of $\bm{\Sigma}$ is the direction that has the smallest reconstruction error as well as the largest variance over the residual data.
\par Repeating the above procedure, i.e. finding the principal eigenvector of $\bm{\Sigma}-\lambda \bm{uu}^T$, which is the 2nd principal eigenvector of $\bm{\Sigma}$, we can iteratively identity the $d$ principal eigenvectors of $\bm{\Sigma}$. Note that the eigenvectors are guaranteed to be pairwise orthogonal.
\par Let us take a matrix view to complement the iterative one. The eigen-decomposition (\ref{eq_2_edom}) can also be written in a matrix form (also known as diagonalization):
\begin{equation*}
{\bf \Sigma = U\Lambda U}^T,\ {\bf \Lambda} =\text{diag}(\lambda_1,\dots, \lambda_m),\ \lambda_1\geq\dots\geq\lambda_m\geq 0
\end{equation*}
where ${\bf U}$ is an orthogonal matrix (unit-length and orthogonal columns) whose columns are the eigenvectors of $\bm{\Sigma}$ (forming an eigenvector basis):
\begin{equation*}
{\bf U}=(\bm{u}_1,\dots,\bm{u}_m),\ {\bf U}^T\bm{u}_i=\bm{e}_i,\ \bm{\Sigma u}_i = \lambda_i\bm{u}_i
\end{equation*}
The following are two useful results from linear algebra
\begin{theorem}\label{thm_2_spectral}
	(Spectral Theorem): Matrix ${\bf A}$ is diagonalizable by an orthogonal matrix if and only if it is symmetric.
\end{theorem}
\begin{theorem}
	Distinct eigenvalues of symmetric matrices have orthogonal eigenvectors.
\end{theorem}
\begin{proof}
	\begin{equation*}
	\lambda_j \bm{u}_i^T\bm{u}_j = \bm{u}_i^T{\bf A}\bm{u}_j = \bm{u}_j^T{\bf A}\bm{u}_i = \lambda_i \bm{u}_j^T\bm{u}_i
	\end{equation*}
	Since $\lambda_i\neq \lambda_j$, we have $\bm{u}_i^T\bm{u}_j=0$.
\end{proof}
\vbox{}
\par Note that $\bm{\Sigma}=\frac{1}{n}{\bf XX^T}$ is symmetric, so by the above two theorems, the optimal reduction to $d$ dimension via PCA is to compute the first $d$ coordinates of the data in the basis formed by eigenvectors of $\bm{\Sigma}$:
\begin{equation*}
{\bf Z}={\bf \tilde{U}}^T{\bf X}\in \mathbb{R}^{d\times n},\ {\bf \tilde{U}} = (\bm{u}_1,\dots,\bm{u}_d)\in\mathbb{R}^{m\times d},\ d\leq m
\end{equation*}
and the optimal reconstruction (guaranteed by Eckart-Young (Theorem \ref{thm_1_EY})) is to project the data to the subspace spanned by the $d$ principal eigenvectors of $\bm{\Sigma}$:
\begin{equation*}
{\bf \tilde{X}}={\bf \tilde{U}Z} = {\bf \tilde{U}\tilde{U}}^T{\bf X}
\end{equation*}
where ${\bf \tilde{U}\tilde{U}}^T$ is the projection matrix.
\section{Algorithms and Interpretation}
\subsection{Power Method}
Eigenvalues and eigenvectors are extremely important for analytic purposes. Sometimes we want to know exactly what the eigenvectors and the eigenvalues are, which calls for the development of efficient algorithms to compute them. Power iteration is a simple algorithm for finding dominant eigenvectors of ${\bf A}$.
\par As suggested by its name, power iteration is done in a iterative way. We start from a random vector $\bm{v}_0$, multiply by ${\bf A}$, normalize it and do it all over again
\begin{equation*}
\bm{v}_{t+1} = \frac{{\bf A}\bm{v}_t}{\|{\bf A}\bm{v}_t\|}
\end{equation*}
where we made two technical assumptions:
\begin{equation}\label{eq_2_asp}
\langle \bm{u}_1,\bm{v}_0\rangle\neq 0\ \text{and}\ |\lambda_1|>|\lambda_j|(\forall j\geq2)
\end{equation}
Note that if we pick $\bm{v}_0$ uniformly random, the first assumption is satisfied with probability 1. The following proposition shows the convergence of Power Iteration for a symmetric p.s.d. ${\bf A}$ when the two assumptions are satisfied.
\begin{proposition}
	For Power Iteration algorithm we discussed above, if $\bm{v}_0$ and ${\bf A}$ satisfy (\ref{eq_2_asp}) then it follows
	\begin{equation*}
	\lim_{t\rightarrow\infty}\bm{v}_t=\bm{u}_1
	\end{equation*}
	and we can also recover $\lambda_1$ from Rayleigh quotient:
	\begin{equation*}
	\lambda_1 = \lim_{t\rightarrow \infty} \frac{\|{\bf A}\bm{v}_t\|}{\|\bm{v}_t\|}
	\end{equation*}
\end{proposition}
\begin{proof}
	Since ${\bf A}$ is p.s.d. and symmetric, by Spectral Theorem (Theorem \ref{thm_2_spectral}) we know it is diagonalizable, and therefore has eigenbasis $\{\bm{u}_1,\dots,\bm{u}_m\}$. We can write $\bm{v}_0$ as a linear combination of $\{\bm{u}_1,\dots,\bm{u}_m\}$:
	\begin{equation*}
	\bm{v}_0 = \sum_{i=1}^{m} \alpha_i \bm{u}_i,\ \alpha\neq0
	\end{equation*}
	With the eigen-decomposition of ${\bf A}$: ${\bf A}= \sum_{i=1}^m \lambda_i\bm{u}_i\bm{u}_i^T $ and using that fact that $\bm{u}_i$'s are orthogonal, we have for any vector $\bm{v} = \sum_{i=1}^{m} w_i \bm{u}_i$
	\begin{equation*}
	{\bf A}\bm{v} = (\sum_{i=1}^m \lambda_i\bm{u}_i\bm{u}_i^T)(\sum_{i=1}^{m} w_i \bm{u}_i) = \sum_{i=1}^{m} \lambda_i w_i \bm{u}_i
	\end{equation*}
	which shows that multiplying a vector by ${\bf A}$ is equivalent to multiplying its coordinates under the basis $\{\bm{u}_1,\dots,\bm{u}_m\}$ by $\lambda_1, ..., \lambda_m$ coordinate-wise. Note that even if we do normalization in each time step (for numerical reason), it does not affect the direction of ${\bf A}\bm{v}_t$, and therefore it is equivalent to normalize only once at the end. Thus, denote the normalization factor in the last step as $c_t$ we obtain
	\begin{equation*}
	\lim_{t\rightarrow \infty}\bm{v}_t = \lim_{t\rightarrow \infty}\frac{1}{c_t}\sum_{i=1}^{m} \lambda_i^t \alpha_i \bm{u}_i = \lim_{t\rightarrow \infty}\frac{\lambda_1^t\alpha_1}{c_t} \left(\bm{u}_1 + \sum_{i=2}^{m} \frac{\alpha_i}{\alpha_1}\left(\frac{\lambda_i}{\lambda_1}\right)^t\bm{u}_i\right) = \bm{u}_1
	\end{equation*}
	as $\lambda_j<\lambda_1$. Note that $c_t$ is a normalization factor, thus $c_t\rightarrow 1/(\lambda_1^t\alpha_1)$.
\end{proof}
\subsection{Example}
Digital images are saved as matrices which can be vectorized and we can then apply PCA to analyze them. The following are the mean vector and first four principal directions (with eigenvalues) of number three in a digital number data set. (Blue means positive and yellow means negative.)
\begin{figure}[h]
	\centering 
	\includegraphics[width=10cm]{fig_2_1.jpg} 
	\caption{Digital number example of PCA}\label{fig_2_1}
\end{figure}
\begin{figure}[h] 
	\centering 
	\includegraphics[width=10cm]{fig_2_2.jpg} 
	\caption{Eigenvalue spectrum example (left), and approximation error (right)}\label{fig_2_2}
\end{figure}
\par We can see that (Figure \ref{fig_2_1}) the first 4 principal eigenvectors have very clear interpretations. The first one mainly deals with tilting, and the 4th one probably handles scaling. Also the eigenvalues show the relative importance of each principal vector.
\par It's also very helpful to look at the eigenvalue spectrum (Figure \ref{fig_2_2}). For structured data like natural images, there is usually a fast decay in its eigenvalue spectrum. Since the reconstruction error depends on the sum of eigenvalues we ignore, there is usually a fast decay in reconstruction loss w.r.t. the number of principal components.
\par In the discussion so far, we always assume that the number of principal components (also the reduced dimension number in Linear Autoencoder) is predefined and fixed. A natural question to ask is how should we choose this number, which is sometimes called \emph{intrinsic dimensionality} . A heuristic strategy is to look at the spectrum of eigenvalues and detect the "knee" or "elbow". However, in practice, there are usually more things like the limit of computational power or storage that determine how we choose the number of reduced dimensions.
\begin{figure}[h] 
	\centering 
	\includegraphics[width=6cm]{fig_2_3.jpg} 
	\caption{Detect "knee" to determine intrinsic dimensionality}\label{fig_2_3}
\end{figure}
\subsection{Comparison with Linear Autoencoder Network}
At this point, it is worth spending some time comparing the two dimension reduction methods we have learned:
\begin{itemize}
	\item PCA clarifies that one should (ideally) center the data, while it's not explicit in Linear Autoencoder.
	\item PCA representation is unique (if no degenerate eigenvalues) and as such is (in principle) interpretable.
	\item Linear autoencoder without weight sharing is highly non-interpretable (lack of identifiability).
	\item Linear autoencoder with weight sharing: ${\bf C=D^T}$ identifies the same subspace of $k$ principal eigenvectors (see discussion after Remark \ref{rmk_1_1}), but axes are non-identifiable. To modify an autoencoder to identify the principal axes, techniques like adding regularization to enforce ordering in the axis can be applied.
	\item General lesson for neural networks based methods: caution with naively interpreting learned (neural) representations.
\end{itemize}
\par For the algorithm used to find the solution:
\begin{itemize}
	\item Computing one principal component at a time via power iterations: good for small $k$, conceptually easy and robust.
	\item Training a linear autoencoder via backpropagation: it's easily extensible and we use stochastic optimization (stochastic gradient descent), which is good for very large datasets and has good convergence properties.
	\item Computing PCA from SVD: good for mid-sized problems, can leverage wealth of numerical techniques for SVD (e.g. QR decomposition)
\end{itemize}
\subsection{PCA via SVD}
The power iteration algorithm and eigen-decompostion, which has time complexity $O(m^3)$ for $\bm{\Sigma}\in\mathbb{R}^{m\times m}$, suffer from large $m$. A way to solve the problem for the case where $m$ is considerably large compared to $n$ is to use SVD, which only takes $O(m^2n)$ for ${\bf X}\in \mathbb{R}^{m\times n}$.
\par To compute the eigen-decomposition of ${\bf AA}^T$ via SVD:
\begin{equation*}
{\bf AA}^T = ({\bf UDV})({\bf VD}^T{\bf U}^T) = {\bf UDI_nD}^T{\bf U}^T = {\bf U\Lambda U}^T
\end{equation*}
where ${\bf \Lambda} = {\bf DD}^T=\text{diag}(\lambda_1,\dots,\lambda_m)\in\mathbb{R}^{m\times m}$ and
\begin{equation*}
\lambda_i=\begin{cases}
\sigma_i^2& \text{for }1\leq i \leq \min\{n,m\}\\
0& \text{for }n< i \leq m
\end{cases}
\end{equation*}
Similarly we have ${\bf A}^T{\bf A}={\bf V\Lambda'V}^T$, where
\begin{equation*}
{\bf \Lambda'} = {\bf D}^T{\bf D} = \text{diag}(\lambda'_1,\dots,\lambda'_n)\in\mathbb{R}^{n\times n},\ \lambda'_i=\begin{cases}
\lambda_i^2& \text{for }1\leq i \leq \min\{n,m\}\\
0& \text{for }m< i \leq n
\end{cases}
\end{equation*}
We see that ${\bf U}$, i.e. the left singular vectors of A, is the eigenvectors of ${\bf AA}^T$, and ${\bf V}$, i.e. the right singular vectors of A, is the eigenvectors of ${\bf A}^T{\bf A}$. ${\bf \Lambda}'$ and ${\bf \Lambda}$ are identical up to zero padding. This shows that we can used the SVD of ${\bf X}$ to compute the eigen-decomposition of ${\bf \Sigma}=\frac{1}{n}{\bf XX}^T$. But when $n \gg m$, SVD is not so competitive. Also note that compared to the SVD method, eigen-decomposition of ${\bf \Sigma}$ suffers from very large and small $\sigma_i$'s in terms of numerical stability, since it involves squares.
\subsection{PCA via Transposed Data Matrix}
Another useful technique to deal with the case where $m\gg n$ is to exploit the connection between the eigenvectors of ${\bf AA}^T$ and ${\bf A}^T{\bf A}$. (This is an exam problem in 2020.) 
\par Given ${\bf A}\in \mathbb{R}^{m\times n}$ with $m\gg n$, our goal is to efficiently compute the eigenvectors of ${\bf AA}^T\in \mathbb{R}^{m\times m}$ which turn out to be closely related to the eigenvectors of ${\bf A}^T{\bf A}\in \mathbb{R}^{n\times n}$. For simplicity, we only look at the first principle eigenvector. With a SVD decomposition ${\bf A}={\bf U\Sigma V}^T$, we have
\begin{equation*}
	{\bf AA}^T = {\bf U\Sigma }^2{\bf U}^T,\quad {\bf A}^T{\bf A} ={\bf V\Sigma }^2{\bf V}^T.
\end{equation*}
This shows that the principle eigenvector of ${\bf AA}^T$ (i.e. ${\bf u}_1$) and the one of ${\bf A}^T{\bf A}$ (i.e. ${\bf v}_1$) correspond to the same eigenvalue $\sigma_1^2$. Now suppose we already have ${\bf v}_1$ and $\sigma_1$, then by
\begin{equation*}
	{\bf A}={\bf U\Sigma V}^T = \sum_i \sigma_i {\bf u}_i{\bf v}_i^T
\end{equation*}
we have
\begin{equation*}
	{\bf Av}_1 = \sum_i \sigma_i {\bf u}_i{\bf v}_i^T{\bf v}_1 =\sigma_1 {\bf u}_1 \Rightarrow {\bf u}_1 =\frac{1}{\sigma_1}{\bf Av}_1.
\end{equation*}
Since $n\ll m$, we can get ${\bf v}_1$ and $\sigma_1$ efficiently via the eigendecomposition of ${\bf A}^T{\bf A}$.
\end{document}